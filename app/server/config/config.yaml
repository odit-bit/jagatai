server:
  address: "127.0.0.1:11823" #default server address listening to
  debug: false

provider: # llm backend
  name: "ollama"
  model: "qwen3:1.7b" #Default
  key: ""
  extra:
    # endpoint: "http://127.0.0.1:11434" #ollama default
    # topP: 0.98
    # topK: 40
    # minP: 0.02
    # temperature: 0.8
#
# tools:
#   - name: "openmeteo"
#     endpoint: "https://api.open-meteo.com"
#     api_key: ""
#   - name: "clock"
#     endpoint: ""
#     api_key: ""
#   - name: "osm"
